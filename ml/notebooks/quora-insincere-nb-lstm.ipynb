{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f89fb1a64f63d3cb1845778718e695d2e5e3c2f"
   },
   "source": [
    "# Quora Insincere Questions Classification (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "165162c4c8adebaf5fc9b86e85a0eff988fa0c07"
   },
   "source": [
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-416e6107aed22920d238a91f3bae6681\" width=\"250px\" alt=\"Quora Logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25d3103dfb31b81013fa4a3ca8b07f2614071896"
   },
   "source": [
    "## Table Of Contents:\n",
    "1. [Challenge Description](#Challenge-Description)\n",
    "2. [Data Files Description](#Data-Files-Description)\n",
    "3. [Import necessary libraries](#Import-necessary-libraries)\n",
    "4. [File Paths](#File-Paths)\n",
    "5. [Helper Methods](#Helper-Methods)\n",
    "6. [Data Wrangling](#Data-Wrangling)\n",
    "7. [Feature Engineering](#Feature-Engineering)\n",
    "8. [Data Preprocessing](#Data-Preprocessing)\n",
    "9. [LSTM](#LSTM)\n",
    "10. [Evaluation](#Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "febd1b97429e5997ce01b18f25d70947fb7887b6"
   },
   "source": [
    "### Challenge Description\n",
    "\n",
    "In this challenge, we have to train a model which is able to detect if a given question in insincere or not. The model should be able if the question is a statement rather than a question that if answered will provide benefit to Quora's online community. We will implement and compare various model and finally pick the highest performing one and deploy it on a live instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e72d87fff6af42c36b35a50af5703c1e15443b6"
   },
   "source": [
    "### Data Files Description\n",
    "\n",
    "Value to be predicted: 0 or 1 for each q_id\n",
    "\n",
    "Data files:\n",
    "* **train.csv**: Contains the training data\n",
    "* **test.csv**: Contains the testing data\n",
    "* **embeddings.zip**: A set of already existing embeddings for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "445677aec882614fdcf5e33c8c3bce80462a1122"
   },
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "0f031d4e082fa5e36862b1a8851ad739d231d0ae"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "c51e71216669186f8fbab180cb0b07cb66ff505a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ac9e04e2eaf81cc8fe0f88dc109c9cfc991cbe8b"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "255eacb1d2134623cf4b6bc7ecf91b5c42d6d19f"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "baee81e14b07064398fa8a56fb247bdd92d04384"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras import Model\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "69bae3a9e1017d95eab6ebdd928522054ef2806d"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "47de3b76162ddc7a1785c4a925f208328c4e8915"
   },
   "outputs": [],
   "source": [
    "# Parameters and definitions\n",
    "RANDOM_SEED = 0\n",
    "VAL_SET_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "96e0dae0b6f3d3eb777e0fb4346da37ce9a4c959"
   },
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbb4282c349d40ef1dd3e22eceaefb5028e44356"
   },
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c73b0d56a143137e5cdb3e4f0f7901ffc0987fb4"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../input/\"\n",
    "TRAIN_SAMPLES = DATA_DIR+\"train.csv\"\n",
    "TEST_SAMPLES = DATA_DIR+\"test.csv\"\n",
    "EMBD_SAMPLES = DATA_DIR+\"embeddings.zip\"\n",
    "SUBMISSION_FILE = DATA_DIR+\"submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f467191eecaaee9b4d99c6cd9e44329448c3da37"
   },
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d7670080c67178cbb698ce77a08ae11159a20198"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Loads the training and testing sets into the memory.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(TRAIN_SAMPLES), pd.read_csv(TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a385d0a5a872f85110d62202d6656e86d5c9344a"
   },
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "2b9e67b4a894ac2605c08e2aa084b38551256369"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11d84f0763fd0aed148af34071221ef91a168fe3"
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ca6673cd9c04a52c59aebe52e8bd83dc93e95331"
   },
   "outputs": [],
   "source": [
    "def build_features(data):\n",
    "    \"\"\"\"\"\"\n",
    "    # Number of words on the data set\n",
    "    data[\"n_words\"] = data[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # Number of unique words on the data set\n",
    "    data[\"uniq_words\"] = data[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # Number of characters on data set\n",
    "    data[\"n_chars\"] = data[\"question_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "    # Number of stopwords on data set\n",
    "#     data[\"n_swords\"] = data[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "    # Number of punctuations on data set\n",
    "    data[\"n_punct\"] = data['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "    # Number of title case words on data set\n",
    "    data[\"n_up_words\"] = data[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "    # Number of title case words on data set\n",
    "    data[\"n_titles\"] = data[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    # Average length of the words on data set\n",
    "    data[\"m_w_len\"] = data[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "55f36a27247397e84f7169cf4bfd556208b02ce4"
   },
   "outputs": [],
   "source": [
    "df_train = build_features(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "8897c455046f484f7e1ef139e88bb49c04bb1bfd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>n_words</th>\n",
       "      <th>uniq_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_punct</th>\n",
       "      <th>n_up_words</th>\n",
       "      <th>n_titles</th>\n",
       "      <th>m_w_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "      <td>1.306122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.187018e-02</td>\n",
       "      <td>1.280361e+01</td>\n",
       "      <td>1.213578e+01</td>\n",
       "      <td>7.067884e+01</td>\n",
       "      <td>1.746492e+00</td>\n",
       "      <td>4.506570e-01</td>\n",
       "      <td>2.121108e+00</td>\n",
       "      <td>4.671008e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.409197e-01</td>\n",
       "      <td>7.052437e+00</td>\n",
       "      <td>6.040779e+00</td>\n",
       "      <td>3.878428e+01</td>\n",
       "      <td>1.672051e+00</td>\n",
       "      <td>8.490158e-01</td>\n",
       "      <td>1.495405e+00</td>\n",
       "      <td>8.187338e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>4.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.111111e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>6.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.600000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>8.500000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.142857e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.340000e+02</td>\n",
       "      <td>9.600000e+01</td>\n",
       "      <td>1.017000e+03</td>\n",
       "      <td>4.110000e+02</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>5.766667e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target       n_words      ...           n_titles       m_w_len\n",
       "count  1.306122e+06  1.306122e+06      ...       1.306122e+06  1.306122e+06\n",
       "mean   6.187018e-02  1.280361e+01      ...       2.121108e+00  4.671008e+00\n",
       "std    2.409197e-01  7.052437e+00      ...       1.495405e+00  8.187338e-01\n",
       "min    0.000000e+00  1.000000e+00      ...       0.000000e+00  1.000000e+00\n",
       "25%    0.000000e+00  8.000000e+00      ...       1.000000e+00  4.111111e+00\n",
       "50%    0.000000e+00  1.100000e+01      ...       2.000000e+00  4.600000e+00\n",
       "75%    0.000000e+00  1.500000e+01      ...       3.000000e+00  5.142857e+00\n",
       "max    1.000000e+00  1.340000e+02      ...       3.700000e+01  5.766667e+01\n",
       "\n",
       "[8 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record the min, max and average value for the new columns (n_words\tuniq_words\tn_chars\tn_punct\tn_up_words\tn_titles\tm_w_len)\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "51d727217d293872bfee70562ca30abfb033052b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>n_words</th>\n",
       "      <th>uniq_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_punct</th>\n",
       "      <th>n_up_words</th>\n",
       "      <th>n_titles</th>\n",
       "      <th>m_w_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid    ...      m_w_len\n",
       "0  00002165364db923c7e6    ...     4.615385\n",
       "1  000032939017120e6e44    ...     4.125000\n",
       "2  0000412ca6e4628ce2cf    ...     5.800000\n",
       "3  000042bf85aa498cd78e    ...     5.444444\n",
       "4  0000455dfa3e01eae3af    ...     4.200000\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sneak peak into the updated training set\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef4e3226766e635c4117b77b791bf564b32c6b7d"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "a803898fb3d4e07ba5c96c91dd3b686c6b24b15b"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    # Convert data set to lowercase\n",
    "    data[\"question_text\"] = data[\"question_text\"].apply(lambda s: s.lower())\n",
    "    \n",
    "    # Remove punctuation from the data set\n",
    "    data[\"question_text\"] = data['question_text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "    # Remove digits from the data set\n",
    "    data[\"question_text\"] = data[\"question_text\"].str.replace('\\d+', '')\n",
    "\n",
    "    # Remove stop words from question text\n",
    "    data[\"question_text\"] = data[\"question_text\"].apply(lambda s: \" \".join([item for item in s.split() if item not in stop_words]))\n",
    "\n",
    "    # Stem words\n",
    "    data[\"question_text\"] = data[\"question_text\"].apply(lambda s: \" \".join([stemmer.stem(w) for w in s.split()]))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "816f907a9894252f3ef99eeeea84481d849612b5"
   },
   "outputs": [],
   "source": [
    "# Preprocess training set\n",
    "df_train = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "b466ca77dddd2107fcbe3dbb48863d9bc1a49dea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>n_words</th>\n",
       "      <th>uniq_words</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>m_w_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>quebec nationalist see provinc nation</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>6.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>adopt dog would encourag peopl adopt shop</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>veloc affect time veloc affect space geometri</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>5.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>otto von guerick use magdeburg hemispher</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>5.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>convert montra helicon mountain bike chang tyre</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>5.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid    ...      m_w_len\n",
       "0  00002165364db923c7e6    ...     6.600000\n",
       "1  000032939017120e6e44    ...     5.000000\n",
       "2  0000412ca6e4628ce2cf    ...     5.571429\n",
       "3  000042bf85aa498cd78e    ...     5.833333\n",
       "4  0000455dfa3e01eae3af    ...     5.857143\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update combinatorial features\n",
    "df_train = build_features(df_train)\n",
    "\n",
    "# Delete not needed columns\n",
    "del df_train[\"n_punct\"]\n",
    "del df_train[\"n_up_words\"]\n",
    "del df_train[\"n_titles\"]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdfabe244dd371db2c9896a0532ed51cf4a95be9"
   },
   "source": [
    "### Prepare validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9d7185e304b9c0a19afc132b26080ae3d64009b9"
   },
   "outputs": [],
   "source": [
    "# Split training set into training and validation sets\n",
    "df_train, df_val = train_test_split(df_train, test_size=VAL_SET_SIZE, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "37cd91a6d394c956fe89c9af479bf8f106545995"
   },
   "source": [
    "### Resources\n",
    "\n",
    "\n",
    "For LSTM: https://www.kaggle.com/sdelecourt/simple-lstm-that-does-the-job\n",
    "\n",
    "For loading embeddings: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ac037022b123f77dea713ac3162ece180bbf9458"
   },
   "source": [
    "### Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "905161223c9f653cfa46f0e95501a7b723d0b96b"
   },
   "outputs": [],
   "source": [
    "# File path of pretrained word embeddings\n",
    "EMB_FILE_PATH = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "64756a86ec97cf30419cc7e7e30e9c7b538846eb"
   },
   "outputs": [],
   "source": [
    "# Load GloVe Word Embeddings\n",
    "def load_embeddings(file_path):\n",
    "    \"\"\" Loads word embeddings and returns embeddings index\n",
    "    \"\"\"\n",
    "    embeddings_index = {}\n",
    "    f = open(file_path)\n",
    "    for line in tqdm(f):\n",
    "        values = line.split(\" \")\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "03076a8d394b81ca851fbe12da382098d036d268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:26, 10652.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb_index = load_embeddings(EMB_FILE_PATH)\n",
    "print('Found %s word vectors.' % len(emb_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "b9c2dbc58142b680e7d24c6425de9166fddd6299"
   },
   "outputs": [],
   "source": [
    "# Extract text and targets from training set\n",
    "train_questions = df_train['question_text'].values\n",
    "y_train = df_train['target'].values\n",
    "\n",
    "# Extract text and targets from validation set\n",
    "val_questions = df_val['question_text'].values\n",
    "y_val = df_val['target'].values\n",
    "\n",
    "# Extract text and targets from test set\n",
    "test_questions = df_test['question_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "1e7144dce026d6e0a1555eec7a9a76bdc9cf2e07"
   },
   "outputs": [],
   "source": [
    "# Number of unique words in our dataset\n",
    "NUM_UNIQUE_WORDS = 1044897\n",
    "# Maximum number of words in a question\n",
    "MAX_WORDS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "7ae149f244cd527d9731b0e57a75d8d58372c6e9"
   },
   "outputs": [],
   "source": [
    "def get_tokenizer(num_unique_words):\n",
    "    \"\"\" Returns tokenizer\n",
    "    \"\"\"\n",
    "    return Tokenizer(num_words=num_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "98c2fc6103600fe13da54a6e82255bf37743fbdc"
   },
   "outputs": [],
   "source": [
    "# Convert questions into vectors of integers using Keras Tokenizer\n",
    "tokenizer = get_tokenizer(NUM_UNIQUE_WORDS)\n",
    "tokenizer.fit_on_texts(list(train_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "c01976e744017f5eb19ba8c2283e842f9c0caab4"
   },
   "outputs": [],
   "source": [
    "# Store tokenizer\n",
    "import pickle\n",
    "\n",
    "with open('LSTM_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "d9244ae8f062270738c964fcaf6d8dd3a6f79fcf"
   },
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_questions)\n",
    "X_val = tokenizer.texts_to_sequences(val_questions)\n",
    "X_test = tokenizer.texts_to_sequences(test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "859b4e3ae1d326d9bfe0bab0568ace588c73ae47"
   },
   "outputs": [],
   "source": [
    "# Pad sequences so that they are all the same length. Questions shorter than maxlen are padded with zeros.\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_WORDS)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=MAX_WORDS)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "e69e2061384e16181b44afc35a209feff204cd1a"
   },
   "outputs": [],
   "source": [
    "# Create word index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "93756cfad8bdaf7e0d81a393a846591611ed756b"
   },
   "outputs": [],
   "source": [
    "# Dimension of embedding matrix\n",
    "EMB_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f64eec00d404e6dd4292d6e26c85d22efe734348"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "226d8820480cef1681926dec74a1cdc143c64971"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMB_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = emb_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "5ec10ee2ed7f8a3cb10ea9dffeae1ff51f6f03d5"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMB_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_WORDS,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "8287ecb22ee41fadab53b82147c9490552490903"
   },
   "outputs": [],
   "source": [
    "lstm_out = 200 # dimensionality of output space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "5f8123d3eeefadb26dd53c3c21078e105dbfba65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(200, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 125, 300)          47323500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 47,724,501\n",
      "Trainable params: 401,001\n",
      "Non-trainable params: 47,323,500\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_out = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "b57fc31a5689a852eee9539819c7a1b51cc52ea9"
   },
   "outputs": [],
   "source": [
    "# # Create model \n",
    "# def create_LSTM(embedding_layer):\n",
    "#     \"\"\" Creates LSTM model with embedding layer, LSTM and dense layer\n",
    "#     \"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(embedding_layer)\n",
    "#     model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "#     model.add(Dense(1,activation='sigmoid'))\n",
    "#     model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "f9c0caa75391a6fade8dad557b105c16585ffe81"
   },
   "outputs": [],
   "source": [
    "# LSTM_model = create_LSTM(emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "2e9466695454a7a7b2bc5ae445c3fac54a45bf85",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1044897 samples, validate on 261225 samples\n",
      "Epoch 1/2\n",
      "1044897/1044897 [==============================] - 592s 566us/step - loss: 0.1394 - acc: 0.9469 - val_loss: 0.1227 - val_acc: 0.9528\n",
      "Epoch 2/2\n",
      "1044897/1044897 [==============================] - 590s 564us/step - loss: 0.1213 - acc: 0.9530 - val_loss: 0.1166 - val_acc: 0.9547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2f4314fd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model to training data\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "          epochs=2, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "85aefa0b189941c7a1fdc4036f0e6ce2d7849a7a"
   },
   "outputs": [],
   "source": [
    "# Save model \n",
    "model.save('LSTM_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "1ad70125af3bd5c987a48a3a24fb0e75035d082d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261225/261225 [==============================] - 932s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for validation set \n",
    "y_pred_val = model.predict(X_val, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "9dd7e88d3ba54f48b9d87cdc3732b37ae3d030d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 339936/1044897 [========>.....................] - ETA: 42:30"
     ]
    }
   ],
   "source": [
    "# Make predictions for training set\n",
    "y_pred_train = model.predict(X_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "6b308c4b5d77cbf405eca8c1fff97c32c3fb6652"
   },
   "outputs": [],
   "source": [
    "# Convert probabilities into predictions for validation set\n",
    "y_te_val = (np.array(y_pred_val) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_uuid": "6cd185348f688d47c599ef501c34c55f57c73116"
   },
   "outputs": [],
   "source": [
    "# Convert probabilities into predictions for training set\n",
    "y_te_train = (np.array(y_pred_train) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19e0fb0267c88a5a03b9635ccb39e00f42ac05cb"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_uuid": "84e4fdaaf120173df4fe069bc9f4faace04a0ee7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_uuid": "68c057a8c27aeccaa636be717f4bf55fc80198e9"
   },
   "outputs": [],
   "source": [
    "def produce_metrics(y, y_pred):\n",
    "    \"\"\"Produces a report containing the accuracy, f1-score, precision and recall metrics.\n",
    "    \n",
    "    Args:\n",
    "        y: The true classification\n",
    "        y_pred: The predicted classification\n",
    "    \"\"\"\n",
    "    print(\"Accuracy: {}, F1 Score: {}, Precision: {}, Recall: {}\".format(accuracy_score(y, y_pred),\n",
    "                                                                     f1_score(y, y_pred, average=\"macro\"),\n",
    "                                                                     precision_score(y, y_pred, average=\"macro\"),\n",
    "                                                                     recall_score(y, y_pred, average=\"macro\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_uuid": "404c38412719c03eabcecdb893820a1f890f9ba3"
   },
   "outputs": [],
   "source": [
    "def produce_classification_report(y, y_pred):\n",
    "    \"\"\"Produces a classification report.\n",
    "    \n",
    "    Args:\n",
    "        y: The true classification\n",
    "        y_pred: The predicted classification\n",
    "    \"\"\"\n",
    "    print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_uuid": "98594181a30d12da6b1bcd4635e0dbd82fdc8a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954717197817973, F1 Score: 0.777098944834635, Precision: 0.8227636210799454, Recall: 0.7442151677611695\n"
     ]
    }
   ],
   "source": [
    "produce_metrics(y_val, y_te_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_uuid": "d1a864776e972fe3bef5e8bf762f59a60c9e7b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555592560797859, F1 Score: 0.7835296667591048, Precision: 0.82829239606895, Recall: 0.7509097990272653\n"
     ]
    }
   ],
   "source": [
    "produce_metrics(y_train, y_te_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_uuid": "104ff56accf0f57649193732969c66180a99d12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98    245149\n",
      "           1       0.68      0.50      0.58     16076\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    261225\n",
      "   macro avg       0.82      0.74      0.78    261225\n",
      "weighted avg       0.95      0.95      0.95    261225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_classification_report(y_val, y_te_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_uuid": "f9939c907ade700a96cc2cea004269435a204642"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98    980163\n",
      "           1       0.69      0.52      0.59     64734\n",
      "\n",
      "   micro avg       0.96      0.96      0.96   1044897\n",
      "   macro avg       0.83      0.75      0.78   1044897\n",
      "weighted avg       0.95      0.96      0.95   1044897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "produce_classification_report(y_train, y_te_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_uuid": "13593b9699f930ff590388fada288805c7cbf3ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954717197817973, F1 Score: 0.777098944834635\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}, F1 Score: {}\".format(accuracy_score(y_val, y_te_val), \n",
    "                                          f1_score(y_val, y_te_val, average=\"macro\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
